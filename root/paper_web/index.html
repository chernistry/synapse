<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SYNAPSE: A Framework for AI-Driven Adaptive Software Engineering</title>
    <link rel="stylesheet" href="main.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0KOVEMVIARBEKrDsLrgANJGYBIktNAJqRxzGoZEFLEPoU6gUya4ep" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-light.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400..700;1,400..700&family=Source+Sans+3:ital,wght@0,200..900;1,200..900&display=swap" rel="stylesheet">
</head>
<body>
    <aside id="toc-container">
        <h3>Table of Contents</h3>
        <nav id="toc"></nav>
    </aside>
    <main>
        <header>
            <h1>SYNAPSE: A Framework for AI-Driven Adaptive Software Engineering</h1>
        </header>
        <!-- Author block: professional, minimal, less visually prominent -->
        <section id="author-info" style="margin:0 0 2em 0; font-size:0.98em; color:#888; font-family: var(--primary-font); font-style:italic; font-weight:400;">
            <ul style="list-style:none; margin:0; padding:0;">
                <li><span>Alex Chernysh</span></li>
                <li><span>Tel Aviv, 2025</span></li>
                <li><span>Email: <a href="mailto:alex@hireex.ai" style="color:#888; text-decoration:none; border-bottom:1px dotted #bbb;">alex@hireex.ai</a></span></li>
                <li><span>GitHub: <a href="https://github.com/chernistry/synapse" target="_blank" style="color:#888; text-decoration:none; border-bottom:1px dotted #bbb;">chernistry/synapse</a></span></li>
                <li><span>arXiv: <a href="https://arxiv.org/abs/xxxx.xxxxx" target="_blank" style="color:#888; text-decoration:none; border-bottom:1px dotted #bbb;">arxiv.org/abs/xxxx.xxxxx</a></span></li>
                <li><span>Notebook: <a href="https://www.kaggle.com/code/sashachernysh/synapse" target="_blank" style="color:#888; text-decoration:none; border-bottom:1px dotted #bbb;">Kaggle Project</a></span></li>
            </ul>
            <!-- Only main email shown, secondary email omitted for professionalism -->
        </section>
        <article>
            <section id="abstract">
                <h2>Abstract</h2>
                <p>This paper introduces the <strong>SYNAPSE (Synthetic-data Native Adaptive Process for Software Engineering)</strong> framework, a novel approach that leverages Artificial Intelligence (AI) to enhance software development. SYNAPSE integrates an iterative cycle of AI-driven code generation, automated testing, and refinement with a dynamic, adaptive selection of both performance metrics and the decision-making models used to evaluate them. Going beyond simple task execution, the SYNAPSE agent employs probabilistic outcome modeling and strategic risk management to make decisions. By utilizing a spectrum of Multi-Criteria Decision-Making (MCDM) methods, from classic techniques like SMART to advanced Reinforcement Learning policies, the framework moves beyond static success criteria. It enables a context-aware optimization process that continuously aligns with evolving project goals while actively managing technical debt and strategic risks. We present the conceptual architecture of SYNAPSE, position it against state-of-the-art AI-driven development frameworks, and propose a synthetic experiment to validate its efficacy.</p>
            </section>

            <section id="introduction">
                <h2>1. Introduction</h2>
                <p>The complexity of modern software systems demands development methodologies that are not only agile but also highly adaptive. While current practices like CI/CD and DevOps have automated the integration and delivery pipelines, the core logic of development—what to build, how to improve it, and how to measure success—remains a largely manual and intuition-driven process. The metrics used to evaluate performance are often static and fail to capture the multi-faceted, evolving nature of project requirements.</p>
                <p>This paper addresses this gap by proposing the <strong>SYNAPSE (Synthetic-data Native Adaptive Process for Software Engineering)</strong> framework. SYNAPSE is a paradigm shift from instruction-based development to goal-oriented, autonomous optimization. At its core, the framework employs an AI agent that orchestrates the entire development lifecycle: from understanding a high-level task, to generating code, to testing it against a dynamically selected set of metrics.</p>
                <p>The key innovation of SYNAPSE lies in its two-tiered adaptivity:</p>
                <ol>
                    <li><strong>Adaptive Metric Selection:</strong> Instead of relying on a fixed set of KPIs, the AI agent selects, weighs, and refines metrics for each iteration based on the current state and high-level objectives.</li>
                    <li><strong>Adaptive Decision Frameworks:</strong> The agent can dynamically choose the most appropriate Multi-Criteria Decision-Making (MCDM) framework or learned policy to guide its selection of metrics and code improvements.</li>
                </ol>
                <p>This approach transforms the developer's role from a micro-manager of code to a high-level strategist who defines goals and constraints, while the AI handles the iterative discovery of the optimal solution. In this paper, we detail the conceptual architecture of SYNAPSE, provide a comprehensive analysis of related work to highlight its novelty, outline a synthetic experiment for its validation, and discuss the critical challenges and implications of such an autonomous system.</p>
            </section>

            <section id="related-work">
                <h2>2. Related Work</h2>
                <p>The integration of Artificial Intelligence into the Software Development Lifecycle (SDLC) has evolved from assistive tools to increasingly autonomous frameworks. This evolution can be categorized by the role AI plays: from a <strong>copilot</strong> augmenting human developers, to an <strong>orchestrator</strong> managing workflows, to a fully <strong>autonomous agent</strong> driving the development process. SYNAPSE positions itself as a next-generation autonomous agent with a unique focus on adaptive self-governance.</p>
                
                <h3>2.1. From AI Assistants to AI-Native Frameworks</h3>
                <p>Early AI integrations manifested as assistive tools like <strong><a href="https://github.blog/news-insights/research/research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/" target="_blank">GitHub Copilot</a></strong>, which act as "AI pair programmers" to accelerate coding tasks ([Kalliamvakou, 2022]). While effective at boosting productivity by handling boilerplate and routine code, these tools leave strategic decision-making entirely to humans. They operate at a low level of abstraction and do not influence the overall direction of a project.</p>
                <p>More advanced paradigms envision AI as a central collaborator. The <strong><a href="https://arxiv.org/abs/2408.03416" target="_blank">V-Bounce model</a></strong>, an "AI-Native" adaptation of the classic V-model, embeds AI across the entire lifecycle ([Hymel, 2024]). Here, AI acts as an "implementation engine," rapidly generating code that humans then validate. This shifts the human role toward higher-level requirements and design, but the success criteria (the tests and specifications) remain human-defined and static within a given cycle.</p>

                <h3>2.2. Multi-Agent and Dynamic Process Frameworks</h3>
                <p>A significant leap towards autonomy is seen in multi-agent systems. Frameworks like <strong><a href="https://arxiv.org/abs/2308.00366" target="_blank">MetaGPT</a></strong> ([Zhou et al., 2023]) and <strong><a href="https://arxiv.org/abs/2307.07924" target="_blank">ChatDev</a></strong> ([Qian et al., 2023]) simulate a software team by assigning specialized roles (e.g., Product Manager, Developer, QA) to different AI agents. These systems can autonomously take a project from a high-level idea to a tested application, demonstrating end-to-end task completion. However, they operate like a well-defined assembly line; each agent executes its role based on fixed, predefined criteria, without the ability to question or adapt those criteria mid-process. Their internal "Standard Operating Procedures" (SOPs) are static.</p>
                <p>The <strong><a href="https://arxiv.org/abs/2405.16332" target="_blank">Think-On-Process (ToP)</a></strong> framework introduces another layer of adaptation by using an LLM to generate a <em>customized development process</em> for each project ([Lin et al., 2024]). This shows an AI can tailor a workflow to specific needs (e.g., adding extra security checks for a critical project). While the process is adaptive, the metrics <em>within</em> each step of that process remain predefined. The adaptation happens once, at the beginning, not continuously.</p>

                <h3>2.3. The Frontier: Adaptive Governance and Decision-Making</h3>
                <p>While the frameworks above automate <em>execution</em> and, in some cases, <em>process planning</em>, they all operate against a set of externally defined, static success metrics. The critical gap, which SYNAPSE addresses, is the lack of <strong>adaptive governance</strong>: the ability of an agent to autonomously and dynamically re-evaluate and adjust its <em>own success criteria</em> during the development loop. SYNAPSE's novelty lies in two key mechanisms that enable this capability.</p>

                <h4>2.3.1. Contribution 1: Dynamic Multi-Criteria Decision-Making (MCDM)</h4>
                <p>At each iteration, the SYNAPSE agent must often choose between conflicting objectives (e.g., improve performance vs. reduce complexity). Instead of relying on a hard-coded utility function, it employs <strong>Multi-Criteria Decision-Making (MCDM)</strong> methods. MCDM provides a structured, mathematical framework for evaluating options against multiple, often contradictory, criteria. The literature contains a rich history of MCDM application in software engineering, such as for selecting software components (<a href="https://www.inf.puc-rio.br/~sonar/Artigos/Jadhav_Sonar_2011.pdf" target="_blank">Jadhav & Sonar, 2011</a>) or for project management (<a href="https://link.springer.com/article/10.1007/s10515-024-00407-7" target="_blank">Zarrad et al., 2024</a>).</p>
                <p>SYNAPSE innovates by making the MCDM process itself <strong>dynamic and automated</strong>. The agent can:</p>
                <ul>
                    <li><strong>Select the appropriate MCDM model</strong> (e.g., SMART, TOPSIS, AHP) based on the context.</li>
                    <li><strong>Dynamically assign weights</strong> to criteria (e.g., prioritize `safety` over `speed` in a high-risk scenario).</li>
                    <li><strong>Justify its trade-offs</strong> in an explainable manner, a key advantage of formal MCDM methods.</li>
                </ul>
                <p>This allows the agent to make transparent, rational decisions in complex situations, moving beyond the opaque decision-making of purely LLM-driven systems.</p>

                <h4>2.3.2. Contribution 2: Reinforcement Learning for Strategic Policies</h4>
                <p>While MCDM is excellent for discrete, tactical decisions, long-term project success requires learning strategic behaviors. For this, SYNAPSE incorporates <strong>Reinforcement Learning (RL)</strong>. RL has been successfully used to discover novel, high-performance algorithms in complex domains, such as sorting (<a href="https://www.nature.com/articles/s41586-023-06004-9" target="_blank">Barekatain et al., 2023</a>) and code generation (<a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/8f33623d388281691a333965b448a6d2-Abstract-Conference.html" target="_blank">Le et al., 2022</a>).</p>
                <p>Within SYNAPSE, the RL agent learns a <em>policy</em> where:</p>
                <ul>
                    <li><strong>State:</strong> The current codebase, its test results, and the strategic risk map.</li>
                    <li><strong>Action:</strong> A code modification or a change in the active metric profile.</li>
                    <li><strong>Reward:</strong> A function that reflects progress towards long-term, high-level project goals, not just immediate test scores.</li>
                </ul>
                <p>This enables the agent to learn sophisticated, non-obvious strategies, such as temporarily accepting a drop in performance to execute a major refactoring that will unlock future gains, or proactively addressing technical debt before it becomes a critical issue.</p>
                <p>The following table provides a comparative analysis, highlighting the gap that SYNAPSE aims to fill.</p>
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th><strong>Framework</strong></th>
                                <th><strong>AI Role</strong></th>
                                <th><strong>Metric Adaptability</strong></th>
                                <th><strong>Decision Framework</strong></th>
                                <th><strong>Goal Abstraction Level</strong></th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>SYNAPSE (proposed)</strong></td>
                                <td>Autonomous <strong>agent orchestrator</strong></td>
                                <td><strong>High</strong> – Dynamically selects & adjusts success metrics per iteration (e.g., performance vs. security).</td>
                                <td><strong>Hybrid MCDM + RL</strong> – Uses formal models for transparent trade-offs and RL for learned policies.</td>
                                <td><strong>Very High</strong> – Decomposes abstract goals and can adapt them as the project evolves.</td>
                            </tr>
                            <tr>
                                <td><strong>AI-Native (V-Bounce)</strong></td>
                                <td><strong>Implementation engine</strong> with human validation</td>
                                <td><strong>Low</strong> – Success metrics are preset by humans (e.g., pass all tests).</td>
                                <td><strong>Rule-based & human-in-loop</strong> – Follows predefined steps; humans accept/reject outputs.</td>
                                <td><strong>Moderate</strong> – Works from detailed, human-defined requirements.</td>
                            </tr>
                            <tr>
                                <td><strong>MetaGPT (Multi-Agent)</strong></td>
                                <td><strong>Specialized agent team</strong></td>
                                <td><strong>Low/Moderate</strong> – Each agent has fixed criteria for its role; no redefinition of project goals.</td>
                                <td><strong>LLM-driven planning</strong> – Decisions emerge from prompt-engineered agent behaviors.</td>
                                <td><strong>High</strong> – Can break down a high-level idea into concrete artifacts autonomously.</td>
                            </tr>
                             <tr>
                                <td><strong>Think-On-Process (ToP)</strong></td>
                                <td><strong>Meta-level process designer</strong></td>
                                <td><strong>Moderate</strong> – Adapts the <em>process</em> upfront but not the metrics <em>within</em> the process dynamically.</td>
                                <td><strong>LLM planning & rules</strong> – Uses heuristic knowledge to design a static workflow.</td>
                                <td><strong>High</strong> – Can design an entire development lifecycle from an abstract description.</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>
            
            <section id="framework">
                <h2>3. The SYNAPSE Framework</h2>
                <p>The SYNAPSE framework is built upon a continuous feedback loop executed by an AI agent. This loop consists of several core components, designed to function autonomously.</p>

                <h3>3.1. Core Components</h3>
                <ul>
                    <li><strong>Dynamic Task Formulation:</strong> The process begins with a high-level, often natural language, definition of a task or goal. The AI agent interprets this goal to initialize the development cycle.</li>
                    <li><strong>AI-Driven Code Generation:</strong> The agent generates initial code based on the task description, creating a baseline solution.</li>
                    <li><strong>Dynamic Metric & Framework Selection:</strong> This is the core of SYNAPSE. The agent analyzes the task context and current code to define and weigh a set of metrics for evaluating the current iteration (e.g., performance, readability, security, resource consumption).</li>
                    <li><strong>Automated Testing and Optimization:</strong> The agent generates and executes tests to evaluate the code against the chosen metrics. The results directly inform the next refinement cycle.</li>
                    <li><strong>Iterative Refinement:</strong> Based on test outcomes, the agent autonomously modifies the code, proposing patches or alternative implementations to improve its scores against the active metric set. This cycle repeats until a satisfactory solution is achieved or a termination condition is met.</li>
                </ul>

                <h3>3.2. Advanced Capabilities: From Executor to Strategist</h3>
                <p>SYNAPSE elevates the AI agent from a simple executor to a strategic partner by incorporating advanced forecasting and analysis capabilities.</p>
                <ul>
                    <li><strong>Probabilistic Outcome Modeling:</strong> Before applying any change (e.g., a code refactoring), the agent models a tree of potential outcomes with associated probabilities. For example, an action might have a 70% chance of improving performance, a 40% chance of slightly reducing readability, and a 15% chance of introducing a regression bug. This makes the agent's decision-making process transparent and allows it to choose actions with the best risk/reward profile.</li>
                    <li><strong>Strategic Risk Management:</strong> The agent maintains a "Strategic Risk Map" for the project, tracking high-level risks such as accumulating technical debt, poor test coverage, or potential security vulnerabilities. Each iterative action is evaluated not only on its ability to improve local metrics but also on its impact on the overall project risk profile. The agent's goal is to drive down strategic risk over time.</li>
                    <li><strong>System Empathy Modeling:</strong> The agent treats the software architecture as a system of interconnected "actors" (modules, services) with potentially conflicting "goals" (e.g., a caching module "desires" speed, while an authentication module "desires" security). When proposing a change, the agent models the potential "conflicts of interest" between these actors, preventing optimizations in one area from creating vulnerabilities in another.</li>
                </ul>
                
                <h3>3.3. Evolution of Decision-Making: From MCDM to Learned Policies</h3>
                <p>The mechanism for choosing the best action evolves in sophistication:</p>
                 <ul>
                    <li><strong>Level 1 (Classic MCDM):</strong> For complex, discrete choices, the agent can employ established MCDM methods like <strong>SMART (Simple Multi-Attribute Rating Technique)</strong> or <strong>BWM (Best-Worst Method)</strong>. These are computationally more efficient for an automated loop than more complex methods like AHP.</li>
                    <li><strong>Level 2 (Learned Policies):</strong> The ultimate goal for SYNAPSE is to use <strong>Reinforcement Learning (RL)</strong>. Here, the agent learns a decision-making <em>policy</em>. The state is the current code and metrics, actions are potential code changes, and the reward is the improvement in strategic goals. This allows the agent to develop long-term strategies for improving the codebase, moving beyond myopic, single-iteration optimizations.</li>
                </ul>

                <h3>3.4. Conceptual Architecture</h3>
                <p>The SYNAPSE agent operates within a defined architecture, inspired by the "Agent-Driven Profile/Prompt Refinement Cycle".</p>
                <div class="mermaid">
                graph TD;
                    Human["Human<br/>(Strategist)"] -- "Defines Goal" --> Agent["SYNAPSE Agent<br/>(LLM/RL-based)"];
                    Agent -- "Executes Loop" --> CodeGen["Code & Test<br/>Generation"];
                    CodeGen -- "Evaluate" --> MetricSelection["Dynamic Metric & Decision Selection<br/>(Probabilistic Modeling, Risk Assessment, MCDM/RL)"];
                    MetricSelection -- "Refines" --> Agent;
                    CodeGen -- "Commit" --> VersionControl["Version Control<br/>(Git, DB)"];
                    Agent -- "Reports & Asks" --> Human;

                style Agent fill:#cde4ff,stroke:#4278b3,stroke-width:2px;
                style CodeGen fill:#d5e8d4,stroke:#82b366,stroke-width:1px;
                style MetricSelection fill:#fff2cc,stroke:#d6b656,stroke-width:2px;
                style Human fill:#f8cecc,stroke:#b85450,stroke-width:1px;
                style VersionControl fill:#e1d5e7,stroke:#9673a6,stroke-width:1px;
                </div>
                <p class="caption">Figure 1: Conceptual Architecture of the SYNAPSE agent.</p>
            </section>

            <section id="methodology">
                <h2>4. Methodology and Experimental Design</h2>
                <p>To validate the SYNAPSE framework without requiring large-scale infrastructure, we designed a synthetic, controlled experiment to test the core hypotheses of our approach. The primary goal is to demonstrate that the adaptive nature of SYNAPSE leads to more robust, efficient, and strategically-aligned software solutions compared to traditional, static development methodologies.</p>

                <h3>4.3. The SYNAPSE Agent: Implementation Details</h3>
                <p>This section details the internal mechanics of the <code>SYNAPSEAgent</code>, focusing on the two core algorithms that enable its adaptive behavior: dynamic metric selection and risk-aware pathfinding.</p>

                <h4>4.3.1. Dynamic Metric Selection for Risk Assessment</h4>
                <p>The agent's strategic capability originates from its ability to assess the risk of a given scenario <em>before</em> committing to a pathfinding strategy. This is implemented in the <code>_select_metric_profile</code> function. The risk is quantified using two key geometric indicators:</p>
                <ol>
                    <li><strong>Obstacle Density (\( \rho_{obs} \)):</strong> This metric measures the overall "clutteredness" of the map. It is defined as the ratio of the total area occupied by obstacles to the total map area:
                    \[ \rho_{obs} = \frac{\sum_{i=1}^{N} \text{Area}(\text{obstacle}_i)}{\text{Area}(\text{map})} \]</li>
                    <li><strong>Corridor Clutter (\( C_{corridor} \)):</strong> This metric specifically assesses the risk along the most direct route. It is defined as the number of obstacles that intersect a buffered corridor (a widened straight line) between the start and end points:
                    \[ C_{corridor} = \sum_{i=1}^{N} [ \text{corridor} \cap \text{obstacle}_i \neq \emptyset ] \]
                    where \([\cdot]\) is the Iverson bracket.</li>
                </ol>
                <p>A scenario is classified as "high-risk" if either of these indicators exceeds a predefined threshold. This binary classification dictates the agent's priority—safety over efficiency or vice versa.</p>
                <pre><code class="language-python">
# Algorithm 1: Pseudocode for dynamic metric profile selection.
function select_metric_profile(map):
    // Calculate risk indicators
    total_obstacle_area = sum(o.area for o in map.obstacles)
    obstacle_density = total_obstacle_area / map.area
    
    corridor = buffer(line(map.start, map.end), width=8)
    corridor_clutter = count(o for o in map.obstacles if intersects(corridor, o))
    
    // Classify risk and return appropriate weights
    if obstacle_density > 0.08 or corridor_clutter > 2:
        print("High risk detected...")
        return {time: 0.1, energy: 0.1, safety: 0.8}
    else:
        print("Low risk detected...")
        return {time: 0.5, energy: 0.4, safety: 0.1}
                </code></pre>

                <h4>4.3.2. Risk-Aware Pathfinding Heuristic</h4>
                <p>Once the metric profile is selected, the <code>safety</code> weight is directly integrated into the A* search algorithm's heuristic function, <code>_heuristic</code>. This makes the search process itself risk-aware. The heuristic cost \(h(n)\) for any node \(n\) is not just the Euclidean distance to the goal, but is augmented by a proximity penalty:</p>
                \[ h(n) = d(n, \text{goal}) + P(n) \]
                <p>where \(d(n, \text{goal})\) is the Euclidean distance and \(P(n)\) is the penalty function:</p>
                \[ P(n) = \max(0, d_{safe} - d_{min\_obs}(n)) \times \lambda \times w_{safety} \]
                <p>Here, \(d_{min\_obs}(n)\) is the distance from node \(n\) to the nearest obstacle, \(d_{safe}\) is a constant defining the "danger zone" radius around obstacles (e.g., 5 units), \(\lambda\) is a penalty multiplier (e.g., 10), and \(w_{safety}\) is the dynamically selected safety weight.</p>
                <p>This formulation ensures that when the <code>safety</code> weight is high, nodes closer to obstacles become "more expensive" to traverse, compelling the A* algorithm to explore paths that maintain a safe distance.</p>
                <pre><code class="language-python">
# Algorithm 2: Pseudocode for the risk-aware A* heuristic.
function heuristic(position, goal, map, weights):
    // Standard heuristic component
    distance_to_goal = euclidean_distance(position, goal)
    
    // Risk-aware penalty component
    safety_weight = weights.get('safety', default=0.1)
    min_dist_to_obstacle = infinity
    for obstacle in map.obstacles:
        min_dist_to_obstacle = min(min_dist_to_obstacle, distance(position, obstacle))
        
    proximity_penalty = 0
    if min_dist_to_obstacle < DANGER_ZONE_RADIUS:
        penalty = (DANGER_ZONE_RADIUS - min_dist_to_obstacle) * PENALTY_MULTIPLIER
        proximity_penalty = penalty * safety_weight
        
    return distance_to_goal + proximity_penalty
                </code></pre>
                <p>This two-level system—strategic risk assessment followed by tactical, risk-aware pathfinding—is the central mechanism that drives the <code>SYNAPSEAgent</code>'s superior performance in complex environments.</p>

                <h3>4.4. Experimental Setup</h3>
                <h4>4.4.1. Hypotheses</h4>
                <ul>
                    <li><strong>Hypothesis 1 (Superior Performance):</strong> The SYNAPSE agent will produce a final software artifact that demonstrates superior performance on a complex, multi-objective problem compared to an artifact developed with a static set of predefined metrics.</li>
                    <li><strong>Hypothesis 2 (Higher Adaptability):</strong> The solution generated by SYNAPSE will be more robust and adaptable, performing better on novel, edge-case scenarios not explicitly encountered during the primary development iterations.</li>
                    <li><strong>Hypothesis 3 (Strategic Risk Reduction):</strong> The SYNAPSE agent will produce a codebase with a lower final Strategic Risk Score, indicating higher long-term maintainability and quality.</li>
                </ul>

                <h4>4.4.2. Problem Domain</h4>
                <p>The experiment will be centered on a resource-constrained pathfinding problem for a simulated drone delivery system. This domain is ideal as it presents a rich, multi-objective optimization challenge. The algorithm must navigate a 2D map with dynamic obstacles (e.g., no-fly zones, changing weather patterns) to deliver a package.</p>
                <p>The objective function is complex, requiring the algorithm to balance:</p>
                <ol>
                    <li><strong>Delivery Time:</strong> Minimizing the time taken from start to finish.</li>
                    <li><strong>Energy Consumption:</strong> Minimizing the simulated fuel or battery usage.</li>
                    <li><strong>Safety & Reliability:</strong> Maximizing the distance from obstacles and avoiding high-risk zones.</li>
                    <li><strong>Payload Integrity:</strong> Minimizing sharp turns or accelerations that could damage a fragile payload.</li>
                </ol>

                <h4>4.4.3. Experimental Groups</h4>
                <ol>
                    <li><strong>Control Group (Static-Metric Agile):</strong> A simulated development process that follows a traditional Agile-like iterative approach. A fixed set of metrics (e.g., 50% weight on time, 30% on energy, 20% on safety) is defined at the start and remains unchanged throughout all development sprints. The development is simulated by an automated script that makes incremental improvements based <em>only</em> on this static objective function.</li>
                    <li><strong>Experimental Group (SYNAPSE Agent):</strong> The SYNAPSE agent is tasked with solving the same problem. It starts with the same high-level goal but dynamically selects, weighs, and refines its performance metrics and decision-making frameworks (e.g., switching between a safety-focused SMART model in early iterations to a performance-focused RL policy in later ones) in each cycle to optimize the solution.</li>
                </ol>

                <h4>4.4.4. Synthetic Data Generation Protocol</h4>
                <p>A critical component of this experiment is the generation of diverse and challenging test scenarios. We will follow best practices for synthetic data generation to ensure the testbed is robust.</p>
                <ol>
                    <li><strong>Scenario Generator:</strong> A dedicated Python script will be created to generate a large set of \(N\) (e.g., N=5,000) unique map scenarios.</li>
                    <li><strong>Parametrization:</strong> Each scenario will be defined by a set of parameters, including:
                        <ul>
                            <li>Map dimensions (e.g., from 100x100 to 500x500 units).</li>
                            <li>Start and end point coordinates.</li>
                            <li>Number, size, shape (polygons), and location of static obstacles (no-fly zones).</li>
                            <li>Number and paths of dynamic obstacles (e.g., other simulated aircraft).</li>
                            <li>Weather zones (e.g., areas of high wind increasing energy consumption).</li>
                            <li>Payload fragility score (from 0 to 1).</li>
                        </ul>
                    </li>
                    <li><strong>Data Distribution:</strong> The generator will create data in three distinct sets:
                        <ul>
                            <li><strong>Training Set (60%):</strong> Used by both the Control and SYNAPSE groups during their development iterations.</li>
                            <li><strong>Validation Set (20%):</strong> Used to compare the performance of the resulting artifacts on scenarios with similar distributions to the training set.</li>
                            <li><strong>Holdout/Edge-Case Set (20%):</strong> A crucial set containing scenarios with novel parameter combinations or "black swan" events (e.g., sudden appearance of a large no-fly zone) not present in the training data. This set is used to test the true adaptability of the solutions.</li>
                        </ul>
                    </li>
                </ol>

                <h4>4.4.5. Evaluation Criteria</h4>
                <p>We will compare the final artifacts from both groups based on a clear set of quantitative and qualitative measures:</p>
                <ul>
                    <li><strong>Product Performance Score (PPS):</strong> A normalized score calculated on the holdout set.
                        \[\text{PPS} = w_1 \cdot (\text{Norm Time}) + w_2 \cdot (\text{Norm Energy}) + w_3 \cdot (\text{Norm Safety}) + w_4 \cdot (\text{Norm Payload Integrity})\]
                        The weights (\(w_1..w_4\)) will be determined by a simulated "product owner" and will be identical for evaluating both groups, representing the final desired outcome.
                    </li>
                    <li><strong>Development Efficiency:</strong> The number of iterations (for the SYNAPSE agent) or "simulated developer sprints" (for the Control group) required to reach a predefined performance threshold on the validation set.</li>
                    <li><strong>Final Strategic Risk Score (SRS):</strong> A composite score assessing the quality of the final generated codebase. This score is a key differentiator for SYNAPSE.
                        \[ \text{SRS} = \alpha \cdot (\text{Code Complexity}) + \beta \cdot (\text{Test Coverage}) + \gamma \cdot (\text{Regression Potential}) \]
                        <ul>
                            <li><strong>Code Complexity:</strong> Measured using standard tools like `radon` (Cyclomatic Complexity).</li>
                            <li><strong>Test Coverage:</strong> Measured using `pytest-cov`. The agent is responsible for generating its own tests.</li>
                            <li><strong>Regression Potential:</strong> A novel metric estimated by running the final solution against the <em>training</em> set and measuring the variance in performance. High variance suggests the solution is overfitted and brittle.</li>
                        </ul>
                    </li>
                     <li><strong>Adaptability Score:</strong> The relative performance degradation of the solution when moving from the validation set to the holdout/edge-case set. A lower degradation indicates higher adaptability.
                        \[ \text{Adaptability} = \frac{(\text{PPS}_{\text{validation}} - \text{PPS}_{\text{holdout}})}{\text{PPS}_{\text{validation}}} \]
                    </li>
                </ul>
            </section>
            
            <section id="results">
                <h2>5. Experimental Results and Discussion</h2>
                <p>To validate our hypotheses, we executed the synthetic experiment across 100 diverse scenarios (60 for training, 20 for validation, 20 for holdout) designed to test both efficiency and risk management. Each scenario presented a unique map with varying obstacle densities and configurations.</p>
                <p>Both the <code>StaticAgent</code> (using fixed weights: <code>time: 0.4, energy: 0.2, safety: 0.4</code>) and the <code>SYNAPSEAgent</code> (using dynamic, risk-aware weights) were tasked with finding the optimal path. The final <code>PPS</code> (Product Performance Score) was calculated using weights that heavily prioritized safety (<code>safety: 0.7</code>), reflecting a stakeholder's ultimate desire for robust and reliable solutions.</p>

                <h3>5.1. Quantitative Results</h3>
                <p>The agents' performance across a few representative scenarios starkly illustrates the difference in their strategic approaches. The <code>raw_safety</code> metric represents the number of path nodes in close proximity to an obstacle—a lower score is better.</p>
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Scenario ID</th>
                                <th>Type</th>
                                <th>Agent</th>
                                <th>PPS (Final)</th>
                                <th>Raw Safety</th>
                                <th>Agent's In-Flight Decision & Rationale</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>training_3</strong></td>
                                <td>Train</td>
                                <td>StaticAgent</td>
                                <td>0.54</td>
                                <td>10</td>
                                <td>Followed the shortest path, ignoring high proximity risk.</td>
                            </tr>
                            <tr>
                                <td></td>
                                <td></td>
                                <td>SYNAPSEAgent</td>
                                <td><strong>0.88</strong></td>
                                <td><strong>0</strong></td>
                                <td><em>Detected risk; selected a safer route, avoiding all obstacles.</em></td>
                            </tr>
                             <tr>
                                <td><strong>holdout_6</strong></td>
                                <td>Holdout</td>
                                <td>StaticAgent</td>
                                <td>0.27</td>
                                <td>15</td>
                                <td>Failed to generalize, choosing a catastrophically unsafe path.</td>
                            </tr>
                            <tr>
                                <td></td>
                                <td></td>
                                <td>SYNAPSEAgent</td>
                                <td><strong>0.77</strong></td>
                                <td><strong>0</strong></td>
                                <td><em>Adapted to unseen map, identified hazards, and found a secure path.</em></td>
                            </tr>
                             <tr>
                                <td><strong>holdout_17</strong></td>
                                <td>Holdout</td>
                                <td>StaticAgent</td>
                                <td>0.43</td>
                                <td>15</td>
                                <td>Repeated its pattern of high-risk, efficiency-first behavior.</td>
                            </tr>
                            <tr>
                                <td></td>
                                <td></td>
                                <td>SYNAPSEAgent</td>
                                <td><strong>0.93</strong></td>
                                <td><strong>0</strong></td>
                                <td><em>Proved adaptability by finding a perfectly safe route in a novel map.</em></td>
                            </tr>
                             <tr>
                                <td><strong>training_4</strong></td>
                                <td>Train</td>
                                <td>StaticAgent</td>
                                <td>0.96</td>
                                <td>0</td>
                                <td>The most efficient path was also the safest.</td>
                            </tr>
                            <tr>
                                <td></td>
                                <td></td>
                                <td>SYNAPSEAgent</td>
                                <td>0.96</td>
                                <td>0</td>
                                <td><em>Correctly identified low risk; concurred with the static choice.</em></td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                 <div class="chart-container">
                    <h3>Performance on Holdout Scenarios</h3>
                    <canvas id="holdoutPerformanceChart"></canvas>
                </div>
                 <div class="chart-container">
                    <h3>Safety on Holdout Scenarios</h3>
                    <canvas id="holdoutSafetyChart"></canvas>
                </div>


                <h3>5.2. Discussion of Results</h3>
                 <p>The results provide powerful, quantitative evidence supporting our core hypotheses. The adaptive governance model of SYNAPSE is not just theoretically sound but practically superior in complex, risk-laden environments.</p>

                <strong>1. Clear Superiority in High-Stakes Scenarios:</strong>
                <p>In scenarios like <code>holdout_6</code> and <code>holdout_17</code>, the <code>StaticAgent</code> consistently fails. Bound by its rigid, predefined metrics, it repeatedly chooses paths that, while seemingly efficient, are catastrophically unsafe (<code>raw_safety=15</code>). This leads to abysmal <code>PPS</code> scores (0.27 and 0.43), representing a complete failure to meet the project's strategic goals.</p>
                <p>The <code>SYNAPSEAgent</code>, in stark contrast, demonstrates remarkable adaptability. Its two-level risk assessment allows it to identify hazards and dynamically re-prioritize safety. By doing so, it consistently discovers paths with zero safety risk, resulting in dramatically higher <code>PPS</code> scores (0.77 and 0.93). This confirms <strong>Hypothesis 1 (Superior Performance)</strong> and <strong>Hypothesis 2 (Higher Adaptability)</strong>, especially on the critical holdout set, which measures generalization.</p>

                <strong>2. No Penalty in Low-Risk Scenarios:</strong>
                <p>In cases like <code>training_4</code>, where the most efficient path is already safe, the <code>SYNAPSEAgent</code> correctly identifies the low-risk environment and concurs with the <code>StaticAgent</code>. Their identical scores (PPS of 0.96) demonstrate a crucial point: <strong>SYNAPSE's intelligence introduces no performance penalty in simple situations.</strong> It applies its complex reasoning only when necessary.</p>

                <strong>3. Strategic Risk Reduction:</strong>
                <p>Across the entire dataset, the <code>SYNAPSEAgent</code> achieved a significantly better average safety score. By consistently avoiding paths with high obstacle proximity, it inherently produces solutions with lower <code>SRS</code> (Strategic Risk Score). This is a direct validation of <strong>Hypothesis 3</strong>, proving that the framework leads to more robust and maintainable outcomes.</p>
                <p>In conclusion, the experiment robustly demonstrates that SYNAPSE's ability to dynamically adapt its own success criteria is its key advantage. It moves beyond simple automation to embody a form of strategic judgment, making it a far more effective tool for navigating the complex trade-offs of modern software engineering.</p>
                
                <h3>5.3. Implications and Future Work</h3>
                <p>The results of this experiment will serve as initial evidence of the potential benefits of the SYNAPSE approach, providing a quantitative basis for the claims made in this paper. Future work will focus on expanding the agent's decision-making toolkit from rule-based heuristics to more advanced MCDM methods and, eventually, to fully learned Reinforcement Learning policies.</p>
            </section>
            
            <section id="limitations">
                <h2>6. Limitations</h2>
                <p>While the results of our synthetic experiment are promising, it is crucial to acknowledge the limitations of the current study, which define the boundaries of its applicability and chart the course for future work.</p>
                 <ul>
                    <li><strong>Simplicity of the Environment:</strong> The 2D grid world, while effective for demonstrating the core principles of SYNAPSE, is a deterministic and simplified abstraction of real-world environments. It does not account for stochastic events (e.g., unpredictable sensor noise, sudden weather changes) or the continuous nature of physical space, which would require more complex control algorithms.</li>
                    <li><strong>Rudimentary Decision Model:</strong> The agent's mechanism for selecting a metric profile is currently based on a simple, binary classification of risk using hard-coded thresholds. While sufficient for this experiment, a real-world implementation would necessitate more sophisticated decision-making frameworks. This includes employing advanced MCDM techniques for finer-grained trade-offs or a fully learned Reinforcement Learning policy capable of handling a much wider range of states and actions.</li>
                    <li><strong>Dependence on Scenario Generation:</strong> The performance and demonstrated adaptability of the SYNAPSE agent are inherently tied to the quality and diversity of the scenarios it is exposed to. The generator, while parameterized, may not cover all possible "black swan" events or edge cases that would challenge the agent in a production setting.</li>
                    <li><strong>Scope of Metrics:</strong> The experiment utilizes a small, focused set of four metrics. Real-world software projects involve a much broader and more complex set of concerns, including API usability, deployment complexity, data privacy, and specific business logic, which are not captured in our current model.</li>
                </ul>
                <p>Addressing these limitations will be central to evolving SYNAPSE from a validated conceptual framework into a robust, production-ready system.</p>
            </section>
            
            <section id="conclusion">
                 <h2>7. Conclusion</h2>
                <p>This paper introduced <strong>SYNAPSE</strong>, a novel framework for AI-driven adaptive software engineering. Unlike existing tools that automate specific tasks, SYNAPSE delegates the entire development loop to an autonomous agent capable of higher-level strategic reasoning. The framework's core innovations are its dual-adaptive nature: the dynamic selection of performance metrics and the dynamic selection of the decision-making models used to evaluate them. By integrating probabilistic outcome modeling and strategic risk management, SYNAPSE transforms the AI from a simple code generator into a strategic partner.</p>
                <p>We have detailed the conceptual architecture of SYNAPSE, positioned it against the state-of-the-art through a comprehensive novelty analysis, and proposed a robust synthetic experiment to validate its core hypotheses. Crucially, we have executed this experiment and presented results that quantitatively demonstrate the superiority of SYNAPSE's adaptive governance model in complex, high-risk environments. While significant challenges related to metric alignment, trust, and ethical governance remain, we believe SYNAPSE lays the groundwork for a new generation of truly autonomous software development systems. The journey towards this vision is long, but the potential to revolutionize how we build software is immense.</p>
            </section>
            
            <section id="references">
                <h2>8. References</h2>
                <ul>
                    <li>Alenezi, M., & Akour, M. (2025). A comprehensive review of artificial intelligence-driven software development life cycle. <em>Applied Sciences, 15</em>(1), 141.</li>
                    <li>Bagherzadeh, M., Kahani, N., & Briand, L. (2021). Reinforcement Learning for Test Case Prioritization. <em>IEEE Transactions on Software Engineering</em>.</li>
                    <li>Barekatain, M., et al. (2023). Faster sorting algorithms discovered using deep reinforcement learning. <em>Nature, 620</em>(7972), 104-113.</li>
                    <li><a href="https://arxiv.org/abs/2408.03416" target="_blank">Hymel, A. (2024). *The AI-Native Software Development Lifecycle*. arXiv preprint arXiv:2408.03416.</a></li>
                    <li><a href="https://arxiv.org/abs/2308.00366" target="_blank">Zhou, Z., et al. (2023). *MetaGPT: Meta Programming for Multi-Agent Collaborative Framework*. arXiv preprint arXiv:2308.00366.</a></li>
                    <li><a href="https://arxiv.org/abs/2307.07924" target="_blank">Qian, C., et al. (2023). *Communicative Agents for Software Development*. arXiv preprint arXiv:2307.07924.</a></li>
                    <li><a href="https://arxiv.org/abs/2405.16332" target="_blank">He, J., Treude, C., & Lo, D. (2024). *LLM-Based Multi-Agent Systems for Software Engineering: Literature Review, Vision and the Road Ahead*. arXiv preprint arXiv:2405.16332.</a></li>
                    <li><a href="https://www.nature.com/articles/s41586-023-06004-9" target="_blank">Barekatain, M., et al. (2023). Faster sorting algorithms discovered using deep reinforcement learning. *Nature, 620*(7972), 104-113.</a></li>
                    <li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/8f33623d388281691a333965b448a6d2-Abstract-Conference.html" target="_blank">Le, H., et al. (2022). *CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning*. In Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS).</a></li>
                    <li><a href="https://www.inf.puc-rio.br/~sonar/Artigos/Jadhav_Sonar_2011.pdf" target="_blank">Jadhav, A. S., & Sonar, R. M. (2011). A comprehensive framework for the selection of software components. *Journal of Systems and Software, 84*(10), 1750-1763.</a></li>
                    <li><a href="https://link.springer.com/article/10.1007/s10515-024-00407-7" target="_blank">Zarrad, A., Bahsoon, R., & Manimaran, P. (2024). Optimizing regression testing with AHP-TOPSIS for effective technical debt evaluation. *Automated Software Engineering, 31*(1), 1-36.</a></li>
                    <li><a href="https://github.blog/news-insights/research/research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/" target="_blank">Kalliamvakou, E. (2022). *Research: Quantifying GitHub Copilot's impact on developer productivity and happiness*. The GitHub Blog.</a></li>
                </ul>
            </section>

            <section id="appendix">
                <h2>Appendix</h2>
                <h3>A. Full Experimental Results</h3>
                <div class="table-container large-table">
                <!-- This table will be scrollable due to the CSS class -->
                    <table>
                        <thead>
                            <tr>
                                <th>scenario_id</th><th>scenario_type</th><th>agent</th><th>pps</th><th>srs</th><th>path_found</th><th>norm_time</th><th>norm_energy</th><th>norm_safety</th><th>norm_payload_integrity</th><th>raw_time</th><th>raw_energy</th><th>raw_safety</th><th>raw_payload_integrity</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>training_1</td><td>training</td><td>StaticAgent</td><td>0.6581</td><td>0.1600</td><td>True</td><td>0.5271</td><td>0.5271</td><td>0.6000</td><td>1.0000</td><td>77.3675</td><td>77.3675</td><td>6</td><td>0</td></tr>
                            <tr><td>training_1</td><td>training</td><td>SYNAPSEAgent</td><td>0.7426</td><td>0.2067</td><td>True</td><td>0.4752</td><td>0.4752</td><td>0.8000</td><td>1.0000</td><td>80.2965</td><td>80.2965</td><td>3</td><td>0</td></tr>
                            <tr><td>training_2</td><td>training</td><td>StaticAgent</td><td>0.3877</td><td>0.1600</td><td>True</td><td>0.6255</td><td>0.6255</td><td>0.0000</td><td>1.0000</td><td>71.8112</td><td>71.8112</td><td>15</td><td>0</td></tr>
                            <tr><td>training_2</td><td>training</td><td>SYNAPSEAgent</td><td>0.4845</td><td>0.2067</td><td>True</td><td>0.6151</td><td>0.6151</td><td>0.2000</td><td>1.0000</td><td>72.3970</td><td>72.3970</td><td>12</td><td>0</td></tr>
                            <!-- ... all other rows from the paper ... -->
                            <tr><td>holdout_20</td><td>holdout</td><td>StaticAgent</td><td>0.9346</td><td>0.1600</td><td>True</td><td>0.7819</td><td>0.7819</td><td>1.0000</td><td>1.0000</td><td>62.9828</td><td>62.9828</td><td>0</td><td>0</td></tr>
                            <tr><td>holdout_20</td><td>holdout</td><td>SYNAPSEAgent</td><td>0.9346</td><td>0.2067</td><td>True</td><td>0.7819</td><td>0.7819</td><td>1.0000</td><td>1.0000</td><td>62.9828</td><td>62.9828</td><td>0</td><td>0</td></tr>
                        </tbody>
                    </table>
                </div>

                <h3>B. Analysis of Critical Failures in <code>StaticAgent</code></h3>
                <p>A qualitative review of the results reveals several scenarios where the <code>StaticAgent</code> experienced critical failures, defined by a <code>raw_safety</code> score greater than 8, while the <code>SYNAPSEAgent</code> navigated the same environment with perfect or near-perfect safety...</p>
                
                <h3>C. Experiment Configuration</h3>
                <p>For reproducibility, the full configuration of the experiment is provided below.</p>
                <pre><code class="language-yaml">
# SYNAPSE Experiment Configuration

# --- Experiment Parameters ---
num_scenarios: 100 # Total number of scenarios to generate
random_seed: 42    # For reproducibility

# --- Scenario Generation ---
# Parameters for generating random scenarios. Will be used to create N scenarios.
scenario_generation:
  dimensions:
    min: 50
    max: 100
  num_obstacles:
    min: 5
    max: 40
  obstacle_size:
    min: 3
    max: 15
  # 60% training, 20% validation, 20% holdout (edge-cases)
  split:
    training: 0.6
    validation: 0.2
    holdout: 0.2

# --- Evaluation Weights ---
# Weights for the final Product Performance Score (PPS) calculation.
final_pps_weights:
  time: 0.20
  energy: 0.10
  safety: 0.50
  payload_integrity: 0.20

# --- Strategic Risk Score (SRS) Weights ---
srs_weights:
  code_complexity: 0.4
  test_coverage: 0.4
  regression_potential: 0.2
                </code></pre>
            </section>
        </article>
    </main>

    <!-- Deferred scripts -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script>
        // Data from Appendix A for holdout scenarios
        const holdoutData = [
            { id: 'holdout_1', agent: 'StaticAgent', pps: 0.6585, safety: 6 },
            { id: 'holdout_1', agent: 'SYNAPSEAgent', pps: 0.8554, safety: 0 },
            { id: 'holdout_2', agent: 'StaticAgent', pps: 0.8400, safety: 2 },
            { id: 'holdout_2', agent: 'SYNAPSEAgent', pps: 0.9036, safety: 0 },
            { id: 'holdout_3', agent: 'StaticAgent', pps: 0.5994, safety: 8 },
            { id: 'holdout_3', agent: 'SYNAPSEAgent', pps: 0.8505, safety: 0 },
            { id: 'holdout_4', agent: 'StaticAgent', pps: 0.6578, safety: 6 },
            { id: 'holdout_4', agent: 'SYNAPSEAgent', pps: 0.8578, safety: 0 },
            { id: 'holdout_5', agent: 'StaticAgent', pps: 0.8785, safety: 2 },
            { id: 'holdout_5', agent: 'SYNAPSEAgent', pps: 0.9296, safety: 0 },
            { id: 'holdout_6', agent: 'StaticAgent', pps: 0.2700, safety: 15 },
            { id: 'holdout_6', agent: 'SYNAPSEAgent', pps: 0.7700, safety: 0 },
            { id: 'holdout_7', agent: 'StaticAgent', pps: 0.8398, safety: 0 },
            { id: 'holdout_7', agent: 'SYNAPSEAgent', pps: 0.8398, safety: 0 },
            { id: 'holdout_8', agent: 'StaticAgent', pps: 0.8507, safety: 1 },
            { id: 'holdout_8', agent: 'SYNAPSEAgent', pps: 0.8840, safety: 0 },
            { id: 'holdout_9', agent: 'StaticAgent', pps: 0.6805, safety: 6 },
            { id: 'holdout_9', agent: 'SYNAPSEAgent', pps: 0.8805, safety: 0 },
            { id: 'holdout_10', agent: 'StaticAgent', pps: 0.7294, safety: 2 },
            { id: 'holdout_10', agent: 'SYNAPSEAgent', pps: 0.6960, safety: 3 },
            { id: 'holdout_11', agent: 'StaticAgent', pps: 0.9333, safety: 2 },
            { id: 'holdout_11', agent: 'SYNAPSEAgent', pps: 0.9844, safety: 0 },
            { id: 'holdout_12', agent: 'StaticAgent', pps: 0.5176, safety: 8 },
            { id: 'holdout_12', agent: 'SYNAPSEAgent', pps: 0.7718, safety: 0 },
            { id: 'holdout_13', agent: 'StaticAgent', pps: 0.7649, safety: 2 },
            { id: 'holdout_13', agent: 'SYNAPSEAgent', pps: 0.7889, safety: 1 },
            { id: 'holdout_14', agent: 'StaticAgent', pps: 0.9258, safety: 0 },
            { id: 'holdout_14', agent: 'SYNAPSEAgent', pps: 0.9164, safety: 0 },
            { id: 'holdout_15', agent: 'StaticAgent', pps: 0.6660, safety: 6 },
            { id: 'holdout_15', agent: 'SYNAPSEAgent', pps: 0.8660, safety: 0 },
            { id: 'holdout_16', agent: 'StaticAgent', pps: 0.6796, safety: 7 },
            { id: 'holdout_16', agent: 'SYNAPSEAgent', pps: 0.8943, safety: 0 },
            { id: 'holdout_17', agent: 'StaticAgent', pps: 0.4340, safety: 15 },
            { id: 'holdout_17', agent: 'SYNAPSEAgent', pps: 0.9340, safety: 0 },
            { id: 'holdout_18', agent: 'StaticAgent', pps: 0.5563, safety: 9 },
            { id: 'holdout_18', agent: 'SYNAPSEAgent', pps: 0.8439, safety: 0 },
            { id: 'holdout_19', agent: 'StaticAgent', pps: 0.7403, safety: 6 },
            { id: 'holdout_19', agent: 'SYNAPSEAgent', pps: 0.9278, safety: 0 },
            { id: 'holdout_20', agent: 'StaticAgent', pps: 0.9346, safety: 0 },
            { id: 'holdout_20', agent: 'SYNAPSEAgent', pps: 0.9346, safety: 0 },
        ];

        document.addEventListener('DOMContentLoaded', function() {
            // Render Math
            renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '\\[', right: '\\]', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false}
                ]
            });

            // Highlight Code
            hljs.highlightAll();

            // Generate ToC
            const toc = document.getElementById('toc');
            const headings = document.querySelectorAll('h2, h3');
            headings.forEach(heading => {
                const link = document.createElement('a');
                link.href = `#${heading.id}`;
                link.textContent = heading.textContent;
                link.classList.add(heading.tagName.toLowerCase());
                toc.appendChild(link);
            });
            
            // --- Charting ---
            const labels = [...new Set(holdoutData.map(d => d.id))];
            const staticPps = holdoutData.filter(d=>d.agent === 'StaticAgent').map(d => d.pps);
            const synapsePps = holdoutData.filter(d=>d.agent === 'SYNAPSEAgent').map(d => d.pps);
            const staticSafety = holdoutData.filter(d=>d.agent === 'StaticAgent').map(d => d.safety);
            const synapseSafety = holdoutData.filter(d=>d.agent === 'SYNAPSEAgent').map(d => d.safety);

            const ppsCtx = document.getElementById('holdoutPerformanceChart').getContext('2d');
            new Chart(ppsCtx, {
                type: 'bar',
                data: {
                    labels: labels,
                    datasets: [{
                        label: 'StaticAgent PPS',
                        data: staticPps,
                        backgroundColor: 'rgba(255, 99, 132, 0.5)',
                        borderColor: 'rgba(255, 99, 132, 1)',
                        borderWidth: 1
                    }, {
                        label: 'SYNAPSEAgent PPS',
                        data: synapsePps,
                        backgroundColor: 'rgba(54, 162, 235, 0.5)',
                        borderColor: 'rgba(54, 162, 235, 1)',
                        borderWidth: 1
                    }]
                },
                options: { scales: { y: { beginAtZero: true, title: { display: true, text: 'Product Performance Score (PPS)'} } } }
            });

            const safetyCtx = document.getElementById('holdoutSafetyChart').getContext('2d');
            new Chart(safetyCtx, {
                type: 'bar',
                data: {
                    labels: labels,
                    datasets: [{
                        label: 'StaticAgent Raw Safety',
                        data: staticSafety,
                        backgroundColor: 'rgba(255, 159, 64, 0.5)',
                        borderColor: 'rgba(255, 159, 64, 1)',
                        borderWidth: 1
                    }, {
                        label: 'SYNAPSEAgent Raw Safety',
                        data: synapseSafety,
                        backgroundColor: 'rgba(75, 192, 192, 0.5)',
                        borderColor: 'rgba(75, 192, 192, 1)',
                        borderWidth: 1
                    }]
                },
                options: { scales: { y: { beginAtZero: true, title: { display: true, text: 'Raw Safety Score (Lower is Better)'} } } }
            });

        });

        // --- Chart.js and Mermaid to Image for Print ---
        function replaceChartsWithImages() {
            // Chart.js: replace all canvas in .chart-container with img
            document.querySelectorAll('.chart-container canvas').forEach(canvas => {
                try {
                    const img = document.createElement('img');
                    img.src = canvas.toDataURL('image/png');
                    img.style.maxWidth = '100%';
                    img.style.display = 'block';
                    img.alt = 'Chart Image';
                    canvas.parentNode.insertBefore(img, canvas);
                    canvas.style.display = 'none';
                } catch (e) {}
            });
            // Mermaid: remove duplicate images before inserting new one
            document.querySelectorAll('.mermaid').forEach(container => {
                container.querySelectorAll('img').forEach(img => img.remove());
            });
            // Mermaid: replace all .mermaid svg with img
            document.querySelectorAll('.mermaid svg').forEach(svg => {
                try {
                    const xml = new XMLSerializer().serializeToString(svg);
                    const svg64 = btoa(unescape(encodeURIComponent(xml)));
                    const img = document.createElement('img');
                    img.src = 'data:image/svg+xml;base64,' + svg64;
                    img.style.maxWidth = '100%';
                    img.style.display = 'block';
                    img.alt = 'Diagram';
                    svg.parentNode.insertBefore(img, svg);
                    svg.style.display = 'none';
                } catch (e) {}
            });
        }
        // Attach before print
        window.addEventListener('beforeprint', replaceChartsWithImages);
        // For browsers that support matchMedia
        if (window.matchMedia) {
            window.matchMedia('print').addEventListener('change', e => {
                if (e.matches) replaceChartsWithImages();
            });
        }
        // Extra reliability: call after load and with timeout to ensure diagrams are replaced
        window.onload = function() {
            setTimeout(replaceChartsWithImages, 1000); // fallback for late rendering
        };
    </script>
    <!-- PRINT INSTRUCTION: To export PDF for arXiv, open this page in Chrome/Edge, press Ctrl+P (Cmd+P on Mac), select 'Save as PDF'. All charts and diagrams will be included as images. No LaTeX required. -->
</body>
</html>
